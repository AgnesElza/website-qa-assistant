# ğŸ§¾ Website QA Assistant

Ask questions about any article â€” like a Medium post â€” using LangChain, Hugging Face models, and vector search.

This project uses a Retrieval-Augmented Generation (RAG) pipeline to:
- Load content from a public web URL
- Chunk and embed the content
- Store and retrieve context with Chroma
- Generate accurate answers using Hugging Face models (e.g., `flan-alpaca-large`)

---

## ğŸš€ Features

- ğŸ”— Load articles using `UnstructuredURLLoader`  
- âœ‚ï¸ Split content with `RecursiveCharacterTextSplitter` for better context retention  
- ğŸ§  Embed using `all-MiniLM-L6-v2` via `HuggingFaceEmbeddings`  
- ğŸ—ƒ Store chunks in a local ChromaDB instance  
- ğŸ¤– Answer questions using a Hugging Face model via `HuggingFacePipeline`  
- ğŸ“ Custom prompt template for high-quality responses  
- ğŸ“¦ 100% local â€” no OpenAI or hosted API required  

---

## ğŸ“¦ Tech Stack

- [LangChain](https://www.langchain.com/)  
- [Transformers (Hugging Face)](https://huggingface.co/docs/transformers)  
- [Chroma Vector DB](https://www.trychroma.com/)  
- `sentence-transformers`, `unstructured`, `tqdm`, `bs4`

---

## ğŸ›  Setup Instructions

### 1. Clone the repo
```bash
git clone https://github.com/your-username/website-qa-assistant.git
cd website-qa-assistant
```

### 2. Create and activate environment
```bash
conda create -n data-science-env python=3.10
conda activate data-science-env
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ“˜ Example Flow

Paste the article URL:  
`https://medium.com/some-article-link`

ğŸ“ Loading article...  
âœ… Loaded 1 document(s)  
âœ‚ï¸ Splitting document into chunks...  
ğŸ“ Embedding chunks...  
âœ… Embeddings stored and retriever ready!  
ğŸ¤– Loading Hugging Face model...

Ask a question about the article (or type `exit`):  
`What is the summary?`  
ğŸ“– **Answer**: This article explains how data science is evolving, not disappearing...

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py             # Main application script  
â”œâ”€â”€ requirements.txt   # Python dependencies  
â”œâ”€â”€ .env.example       # Template for environment variable setup  
â”œâ”€â”€ README.md          # This file  
â”œâ”€â”€ .gitignore         # Ignores .env, chroma_db, etc.  
â””â”€â”€ chroma_db/         # Auto-generated vector store (excluded from Git)
```

---

## ğŸ” Environment Variables

Create a `.env` file in your project root (not committed) with:

```
HUGGINGFACEHUB_API_TOKEN=your_token_here
```

Only needed if using hosted models (not required in this project).  
Use `.env.example` as a reference.

---

## ğŸ§  Key Concepts

- **LangChain** abstracts LLM workflows and chaining logic  
- **Hugging Face Pipelines** enable local inference without an API  
- **RAG** enriches model outputs with external documents  
- **ChromaDB** makes semantic retrieval fast and simple  
- **Prompt engineering** improves the quality of generated answers

---

## ğŸ“œ License

This project is open-sourced under the [MIT License](LICENSE).

---

## ğŸ™‹â€â™€ï¸ Author

Created by **Agnes A**  
[Portfolio](https://portfolio.agnesaugustine.com/) â€¢ [LinkedIn](https://www.linkedin.com/in/agnesaugustine/) â€¢ [GitHub](https://github.com/AgnesElza)